<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>R: Apply an R Function in Spark</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="R.css" />
</head><body>

<table width="100%" summary="page for spark_apply {sparklyr}"><tr><td>spark_apply {sparklyr}</td><td style="text-align: right;">R Documentation</td></tr></table>

<h2>Apply an R Function in Spark</h2>

<h3>Description</h3>

<p>Applies an R function to a Spark object (typically, a Spark DataFrame).
</p>


<h3>Usage</h3>

<pre>
spark_apply(x, f, columns = NULL, memory = !is.null(name),
  group_by = NULL, packages = NULL, context = NULL, name = NULL,
  ...)
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>x</code></td>
<td>
<p>An object (usually a <code>spark_tbl</code>) coercable to a Spark DataFrame.</p>
</td></tr>
<tr valign="top"><td><code>f</code></td>
<td>
<p>A function that transforms a data frame partition into a data frame.
The function <code>f</code> has signature <code>f(df, context, group1, group2, ...)</code> where
<code>df</code> is a data frame with the data to be processed, <code>context</code>
is an optional object passed as the <code>context</code> parameter and <code>group1</code> to
<code>groupN</code> contain the values of the <code>group_by</code> values. When
<code>group_by</code> is not specified, <code>f</code> takes only one argument.
</p>
<p>Can also be an <code>rlang</code> anonymous function. For example, as <code>~ .x + 1</code>
to define an expression that adds one to the given <code>.x</code> data frame.</p>
</td></tr>
<tr valign="top"><td><code>columns</code></td>
<td>
<p>A vector of column names or a named vector of column types for
the transformed object. When not specified, a sample of 10 rows is taken to
infer out the output columns automatically, to avoid this performance penalty,
specify the the column types. The sample size is confgirable using the
<code>sparklyr.apply.schema.infer</code> configuration option.</p>
</td></tr>
<tr valign="top"><td><code>memory</code></td>
<td>
<p>Boolean; should the table be cached into memory?</p>
</td></tr>
<tr valign="top"><td><code>group_by</code></td>
<td>
<p>Column name used to group by data frame partitions.</p>
</td></tr>
<tr valign="top"><td><code>packages</code></td>
<td>
<p>Boolean to distribute <code>.libPaths()</code> packages to each node,
a list of packages to distribute, or a package bundle created with
<code>spark_apply_bundle()</code>.
</p>
<p>Defaults to <code>TRUE</code> or the <code>sparklyr.apply.packages</code> value set in
<code>spark_config()</code>.
</p>
<p>For clusters using Yarn cluster mode, <code>packages</code> can point to a package
bundle created using <code>spark_apply_bundle()</code> and made available as a Spark
file using <code>config$sparklyr.shell.files</code>. For clusters using Livy, packages
can be manually installed on the driver node.
</p>
<p>For offline clusters where <code>available.packages()</code> is not available,
manually download the packages database from
https://cran.r-project.org/web/packages/packages.rds and set
<code>Sys.setenv(sparklyr.apply.packagesdb = "&lt;pathl-to-rds&gt;")</code>. Otherwise,
all packages will be used by default.
</p>
<p>For clusters where R packages already installed in every worker node,
the <code>spark.r.libpaths</code> config entry can be set in <code>spark_config()</code>
to the local packages library. To specify multiple paths collapse them
(without spaces) with a comma delimiter (e.g., <code>"/lib/path/one,/lib/path/two"</code>).</p>
</td></tr>
<tr valign="top"><td><code>context</code></td>
<td>
<p>Optional object to be serialized and passed back to <code>f()</code>.</p>
</td></tr>
<tr valign="top"><td><code>name</code></td>
<td>
<p>Optional table name while registering the resulting data frame.</p>
</td></tr>
<tr valign="top"><td><code>...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Configuration</h3>

<p><code>spark_config()</code> settings can be specified to change the workers
environment.
</p>
<p>For instance, to set additional environment variables to each
worker node use the <code>sparklyr.apply.env.*</code> config, to launch workers
without <code>--vanilla</code> use <code>sparklyr.apply.options.vanilla</code> set to
<code>FALSE</code>, to run a custom script before launching Rscript use
<code>sparklyr.apply.options.rscript.before</code>.
</p>


<h3>Examples</h3>

<pre>
## Not run: 

library(sparklyr)
sc &lt;- spark_connect(master = "local")

# creates an Spark data frame with 10 elements then multiply times 10 in R
sdf_len(sc, 10) %&gt;% spark_apply(function(df) df * 10)


## End(Not run)

</pre>

<hr /><div style="text-align: center;">[Package <em>sparklyr</em> version 1.0.1 <a href="00Index.html">Index</a>]</div>
</body></html>
