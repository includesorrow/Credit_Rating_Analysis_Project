<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>R: Spark ML - Latent Dirichlet Allocation</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="R.css" />
</head><body>

<table width="100%" summary="page for ml_lda {sparklyr}"><tr><td>ml_lda {sparklyr}</td><td style="text-align: right;">R Documentation</td></tr></table>

<h2>Spark ML &ndash; Latent Dirichlet Allocation</h2>

<h3>Description</h3>

<p>Latent Dirichlet Allocation (LDA), a topic model designed for text documents.
</p>


<h3>Usage</h3>

<pre>
ml_lda(x, formula = NULL, k = 10, max_iter = 20,
  doc_concentration = NULL, topic_concentration = NULL,
  subsampling_rate = 0.05, optimizer = "online",
  checkpoint_interval = 10, keep_last_checkpoint = TRUE,
  learning_decay = 0.51, learning_offset = 1024,
  optimize_doc_concentration = TRUE, seed = NULL,
  features_col = "features",
  topic_distribution_col = "topicDistribution",
  uid = random_string("lda_"), ...)

ml_describe_topics(model, max_terms_per_topic = 10)

ml_log_likelihood(model, dataset)

ml_log_perplexity(model, dataset)

ml_topics_matrix(model)
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>x</code></td>
<td>
<p>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</p>
</td></tr>
<tr valign="top"><td><code>formula</code></td>
<td>
<p>Used when <code>x</code> is a <code>tbl_spark</code>. R formula as a character string or a formula. This is used to transform the input dataframe before fitting, see <a href="ft_r_formula.html">ft_r_formula</a> for details.</p>
</td></tr>
<tr valign="top"><td><code>k</code></td>
<td>
<p>The number of clusters to create</p>
</td></tr>
<tr valign="top"><td><code>max_iter</code></td>
<td>
<p>The maximum number of iterations to use.</p>
</td></tr>
<tr valign="top"><td><code>doc_concentration</code></td>
<td>
<p>Concentration parameter (commonly named &quot;alpha&quot;) for the prior placed on documents' distributions over topics (&quot;theta&quot;). See details.</p>
</td></tr>
<tr valign="top"><td><code>topic_concentration</code></td>
<td>
<p>Concentration parameter (commonly named &quot;beta&quot; or &quot;eta&quot;) for the prior placed on topics' distributions over terms.</p>
</td></tr>
<tr valign="top"><td><code>subsampling_rate</code></td>
<td>
<p>(For Online optimizer only) Fraction of the corpus to be sampled and used in each iteration of mini-batch gradient descent, in range (0, 1]. Note that this should be adjusted in synch with <code>max_iter</code> so the entire corpus is used. Specifically, set both so that maxIterations * miniBatchFraction greater than or equal to 1.</p>
</td></tr>
<tr valign="top"><td><code>optimizer</code></td>
<td>
<p>Optimizer or inference algorithm used to estimate the LDA model. Supported: &quot;online&quot; for Online Variational Bayes (default) and &quot;em&quot; for Expectation-Maximization.</p>
</td></tr>
<tr valign="top"><td><code>checkpoint_interval</code></td>
<td>
<p>Set checkpoint interval (&gt;= 1) or disable checkpoint (-1).
E.g. 10 means that the cache will get checkpointed every 10 iterations, defaults to 10.</p>
</td></tr>
<tr valign="top"><td><code>keep_last_checkpoint</code></td>
<td>
<p>(Spark 2.0.0+) (For EM optimizer only) If using checkpointing, this indicates whether to keep the last checkpoint. If <code>FALSE</code>, then the checkpoint will be deleted. Deleting the checkpoint can cause failures if a data partition is lost, so set this bit with care. Note that checkpoints will be cleaned up via reference counting, regardless.</p>
</td></tr>
<tr valign="top"><td><code>learning_decay</code></td>
<td>
<p>(For Online optimizer only) Learning rate, set as an exponential decay rate. This should be between (0.5, 1.0] to guarantee asymptotic convergence. This is called &quot;kappa&quot; in the Online LDA paper (Hoffman et al., 2010). Default: 0.51, based on Hoffman et al.</p>
</td></tr>
<tr valign="top"><td><code>learning_offset</code></td>
<td>
<p>(For Online optimizer only) A (positive) learning parameter that downweights early iterations. Larger values make early iterations count less. This is called &quot;tau0&quot; in the Online LDA paper (Hoffman et al., 2010) Default: 1024, following Hoffman et al.</p>
</td></tr>
<tr valign="top"><td><code>optimize_doc_concentration</code></td>
<td>
<p>(For Online optimizer only) Indicates whether the <code>doc_concentration</code> (Dirichlet parameter for document-topic distribution) will be optimized during training. Setting this to true will make the model more expressive and fit the training data better. Default: <code>FALSE</code></p>
</td></tr>
<tr valign="top"><td><code>seed</code></td>
<td>
<p>A random seed. Set this value if you need your results to be
reproducible across repeated calls.</p>
</td></tr>
<tr valign="top"><td><code>features_col</code></td>
<td>
<p>Features column name, as a length-one character vector. The column should be single vector column of numeric values. Usually this column is output by <code><a href="ft_r_formula.html">ft_r_formula</a></code>.</p>
</td></tr>
<tr valign="top"><td><code>topic_distribution_col</code></td>
<td>
<p>Output column with estimates of the topic mixture distribution for each document (often called &quot;theta&quot; in the literature). Returns a vector of zeros for an empty document.</p>
</td></tr>
<tr valign="top"><td><code>uid</code></td>
<td>
<p>A character string used to uniquely identify the ML estimator.</p>
</td></tr>
<tr valign="top"><td><code>...</code></td>
<td>
<p>Optional arguments, see Details.</p>
</td></tr>
<tr valign="top"><td><code>model</code></td>
<td>
<p>A fitted LDA model returned by <code>ml_lda()</code>.</p>
</td></tr>
<tr valign="top"><td><code>max_terms_per_topic</code></td>
<td>
<p>Maximum number of terms to collect for each topic. Default value of 10.</p>
</td></tr>
<tr valign="top"><td><code>dataset</code></td>
<td>
<p>test corpus to use for calculating log likelihood or log perplexity</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For 'ml_lda.tbl_spark' with the formula interface, you can specify named arguments in '...' that will
be passed 'ft_regex_tokenizer()', 'ft_stop_words_remover()', and 'ft_count_vectorizer()'. For example, to increase the
default 'min_token_length', you can use 'ml_lda(dataset, ~ text, min_token_length = 4)'.
</p>
<p>Terminology for LDA:
</p>

<ul>
<li><p> &quot;term&quot; = &quot;word&quot;: an element of the vocabulary
</p>
</li>
<li><p> &quot;token&quot;: instance of a term appearing in a document
</p>
</li>
<li><p> &quot;topic&quot;: multinomial distribution over terms representing some concept
</p>
</li>
<li><p> &quot;document&quot;: one piece of text, corresponding to one row in the input data
</p>
</li></ul>

<p>Original LDA paper (journal version): Blei, Ng, and Jordan. &quot;Latent Dirichlet Allocation.&quot; JMLR, 2003.
</p>
<p>Input data (<code>features_col</code>): LDA is given a collection of documents as input data, via the <code>features_col</code> parameter. Each document is specified as a Vector of length <code>vocab_size</code>, where each entry is the count for the corresponding term (word) in the document. Feature transformers such as <code><a href="ft_tokenizer.html">ft_tokenizer</a></code> and <code><a href="ft_count_vectorizer.html">ft_count_vectorizer</a></code> can be useful for converting text to word count vectors
</p>


<h3>Value</h3>

<p>The object returned depends on the class of <code>x</code>.
</p>

<ul>
<li> <p><code>spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns an instance of a <code>ml_estimator</code> object. The object contains a pointer to
a Spark <code>Estimator</code> object and can be used to compose
<code>Pipeline</code> objects.
</p>
</li>
<li> <p><code>ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
the clustering estimator appended to the pipeline.
</p>
</li>
<li> <p><code>tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, an estimator is constructed then
immediately fit with the input <code>tbl_spark</code>, returning a clustering model.
</p>
</li>
<li> <p><code>tbl_spark</code>, with <code>formula</code> or <code>features</code> specified: When <code>formula</code>
is specified, the input <code>tbl_spark</code> is first transformed using a
<code>RFormula</code> transformer before being fit by
the estimator. The object returned in this case is a <code>ml_model</code> which is a
wrapper of a <code>ml_pipeline_model</code>. This signature does not apply to <code>ml_lda()</code>.
</p>
</li></ul>

<p><code>ml_describe_topics</code> returns a DataFrame with topics and their top-weighted terms.
</p>
<p><code>ml_log_likelihood</code> calculates a lower bound on the log likelihood of
the entire corpus
</p>


<h3>Parameter details</h3>



<h4><code>doc_concentration</code></h4>

<p>This is the parameter to a Dirichlet distribution, where larger values mean more smoothing (more regularization). If not set by the user, then <code>doc_concentration</code> is set automatically. If set to singleton vector [alpha], then alpha is replicated to a vector of length k in fitting. Otherwise, the <code>doc_concentration</code> vector must be length k. (default = automatic)
</p>
<p>Optimizer-specific parameter settings:
</p>
<p>EM
</p>

<ul>
<li><p> Currently only supports symmetric distributions, so all values in the vector should be the same.
</p>
</li>
<li><p> Values should be greater than 1.0
</p>
</li>
<li><p> default = uniformly (50 / k) + 1, where 50/k is common in LDA libraries and +1 follows from Asuncion et al. (2009), who recommend a +1 adjustment for EM.
</p>
</li></ul>

<p>Online
</p>

<ul>
<li><p> Values should be greater than or equal to 0
</p>
</li>
<li><p> default = uniformly (1.0 / k), following the implementation from <a href="https://github.com/Blei-Lab/onlineldavb">here</a>
</p>
</li></ul>




<h4><code>topic_concentration</code></h4>

<p>This is the parameter to a symmetric Dirichlet distribution.
</p>
<p>Note: The topics' distributions over terms are called &quot;beta&quot; in the original LDA paper by Blei et al., but are called &quot;phi&quot; in many later papers such as Asuncion et al., 2009.
</p>
<p>If not set by the user, then <code>topic_concentration</code> is set automatically. (default = automatic)
</p>
<p>Optimizer-specific parameter settings:
</p>
<p>EM
</p>

<ul>
<li><p> Value should be greater than 1.0
</p>
</li>
<li><p> default = 0.1 + 1, where 0.1 gives a small amount of smoothing and +1 follows Asuncion et al. (2009), who recommend a +1 adjustment for EM.
</p>
</li></ul>

<p>Online
</p>

<ul>
<li><p> Value should be greater than or equal to 0
</p>
</li>
<li><p> default = (1.0 / k), following the implementation from <a href="https://github.com/Blei-Lab/onlineldavb">here</a>.
</p>
</li></ul>




<h4><code>topic_distribution_col</code></h4>

<p>This uses a variational approximation following Hoffman et al. (2010), where the approximate distribution is called &quot;gamma.&quot; Technically, this method returns this approximation &quot;gamma&quot; for each document.
</p>



<h3>See Also</h3>

<p>See <a href="http://spark.apache.org/docs/latest/ml-clustering.html">http://spark.apache.org/docs/latest/ml-clustering.html</a> for
more information on the set of clustering algorithms.
</p>
<p>Other ml clustering algorithms: <code><a href="ml_bisecting_kmeans.html">ml_bisecting_kmeans</a></code>,
<code><a href="ml_gaussian_mixture.html">ml_gaussian_mixture</a></code>,
<code><a href="ml_kmeans.html">ml_kmeans</a></code>
</p>


<h3>Examples</h3>

<pre>
## Not run: 
library(janeaustenr)
library(dplyr)
sc &lt;- spark_connect(master = "local")

lines_tbl &lt;- sdf_copy_to(sc,
  austen_books()[c(1:30), ],
  name = "lines_tbl",
  overwrite = TRUE
)

# transform the data in a tidy form
lines_tbl_tidy &lt;- lines_tbl %&gt;%
  ft_tokenizer(
    input_col = "text",
    output_col = "word_list"
  ) %&gt;%
  ft_stop_words_remover(
    input_col = "word_list",
    output_col = "wo_stop_words"
  ) %&gt;%
  mutate(text = explode(wo_stop_words)) %&gt;%
  filter(text != "") %&gt;%
  select(text, book)

lda_model &lt;- lines_tbl_tidy %&gt;%
  ml_lda(~text, k = 4)

# vocabulary and topics
tidy(lda_model)

## End(Not run)

</pre>

<hr /><div style="text-align: center;">[Package <em>sparklyr</em> version 1.0.1 <a href="00Index.html">Index</a>]</div>
</body></html>
