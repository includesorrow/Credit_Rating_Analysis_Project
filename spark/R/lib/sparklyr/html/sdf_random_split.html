<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>R: Partition a Spark Dataframe</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="R.css" />
</head><body>

<table width="100%" summary="page for sdf_random_split {sparklyr}"><tr><td>sdf_random_split {sparklyr}</td><td style="text-align: right;">R Documentation</td></tr></table>

<h2>Partition a Spark Dataframe</h2>

<h3>Description</h3>

<p>Partition a Spark DataFrame into multiple groups. This routine is useful
for splitting a DataFrame into, for example, training and test datasets.
</p>


<h3>Usage</h3>

<pre>
sdf_random_split(x, ..., weights = NULL,
  seed = sample(.Machine$integer.max, 1))

sdf_partition(x, ..., weights = NULL,
  seed = sample(.Machine$integer.max, 1))
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>x</code></td>
<td>
<p>An object coercable to a Spark DataFrame.</p>
</td></tr>
<tr valign="top"><td><code>...</code></td>
<td>
<p>Named parameters, mapping table names to weights. The weights
will be normalized such that they sum to 1.</p>
</td></tr>
<tr valign="top"><td><code>weights</code></td>
<td>
<p>An alternate mechanism for supplying weights &ndash; when
specified, this takes precedence over the <code>...</code> arguments.</p>
</td></tr>
<tr valign="top"><td><code>seed</code></td>
<td>
<p>Random seed to use for randomly partitioning the dataset. Set
this if you want your partitioning to be reproducible on repeated runs.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sampling weights define the probability that a particular observation
will be assigned to a particular partition, not the resulting size of the
partition. This implies that partitioning a DataFrame with, for example,
</p>
<p><code>sdf_random_split(x, training = 0.5, test = 0.5)</code>
</p>
<p>is not guaranteed to produce <code>training</code> and <code>test</code> partitions
of equal size.
</p>


<h3>Value</h3>

<p>An <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> <code>list</code> of <code>tbl_spark</code>s.
</p>


<h3>Transforming Spark DataFrames</h3>

<p>The family of functions prefixed with <code>sdf_</code> generally access the Scala
Spark DataFrame API directly, as opposed to the <code>dplyr</code> interface which
uses Spark SQL. These functions will 'force' any pending SQL in a
<code>dplyr</code> pipeline, such that the resulting <code>tbl_spark</code> object
returned will no longer have the attached 'lazy' SQL operations. Note that
the underlying Spark DataFrame <em>does</em> execute its operations lazily, so
that even though the pending set of operations (currently) are not exposed at
the <span style="font-family: Courier New, Courier; color: #666666;"><b>R</b></span> level, these operations will only be executed when you explicitly
<code>collect()</code> the table.
</p>


<h3>See Also</h3>

<p>Other Spark data frames: <code><a href="sdf_copy_to.html">sdf_copy_to</a></code>,
<code><a href="sdf_register.html">sdf_register</a></code>, <code><a href="sdf_sample.html">sdf_sample</a></code>,
<code><a href="sdf_sort.html">sdf_sort</a></code>
</p>


<h3>Examples</h3>

<pre>
## Not run: 
# randomly partition data into a 'training' and 'test'
# dataset, with 60% of the observations assigned to the
# 'training' dataset, and 40% assigned to the 'test' dataset
data(diamonds, package = "ggplot2")
diamonds_tbl &lt;- copy_to(sc, diamonds, "diamonds")
partitions &lt;- diamonds_tbl %&gt;%
  sdf_random_split(training = 0.6, test = 0.4)
print(partitions)

# alternate way of specifying weights
weights &lt;- c(training = 0.6, test = 0.4)
diamonds_tbl %&gt;% sdf_random_split(weights = weights)

## End(Not run)
</pre>

<hr /><div style="text-align: center;">[Package <em>sparklyr</em> version 1.0.1 <a href="00Index.html">Index</a>]</div>
</body></html>
