<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>R: Read a Text file into a Spark DataFrame</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="R.css" />
</head><body>

<table width="100%" summary="page for spark_read_text {sparklyr}"><tr><td>spark_read_text {sparklyr}</td><td style="text-align: right;">R Documentation</td></tr></table>

<h2>Read a Text file into a Spark DataFrame</h2>

<h3>Description</h3>

<p>Read a text file into a Spark DataFrame.
</p>


<h3>Usage</h3>

<pre>
spark_read_text(sc, name = NULL, path = name, repartition = 0,
  memory = TRUE, overwrite = TRUE, options = list(), whole = FALSE,
  ...)
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>sc</code></td>
<td>
<p>A <code>spark_connection</code>.</p>
</td></tr>
<tr valign="top"><td><code>name</code></td>
<td>
<p>The name to assign to the newly generated table.</p>
</td></tr>
<tr valign="top"><td><code>path</code></td>
<td>
<p>The path to the file. Needs to be accessible from the cluster.
Supports the <span class="samp">"hdfs://"</span>, <span class="samp">"s3a://"</span> and <span class="samp">"file://"</span> protocols.</p>
</td></tr>
<tr valign="top"><td><code>repartition</code></td>
<td>
<p>The number of partitions used to distribute the
generated table. Use 0 (the default) to avoid partitioning.</p>
</td></tr>
<tr valign="top"><td><code>memory</code></td>
<td>
<p>Boolean; should the data be loaded eagerly into memory? (That
is, should the table be cached?)</p>
</td></tr>
<tr valign="top"><td><code>overwrite</code></td>
<td>
<p>Boolean; overwrite the table with the given name if it
already exists?</p>
</td></tr>
<tr valign="top"><td><code>options</code></td>
<td>
<p>A list of strings with additional options.</p>
</td></tr>
<tr valign="top"><td><code>whole</code></td>
<td>
<p>Read the entire text file as a single entry? Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr valign="top"><td><code>...</code></td>
<td>
<p>Optional arguments; currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>You can read data from HDFS (<code>hdfs://</code>), S3 (<code>s3a://</code>), as well as
the local file system (<code>file://</code>).
</p>
<p>If you are reading from a secure S3 bucket be sure to set the following in your spark-defaults.conf
<code>spark.hadoop.fs.s3a.access.key</code>, <code>spark.hadoop.fs.s3a.secret.key</code> or any of the methods outlined in the aws-sdk
documentation <a href="http://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html">Working with AWS credentials</a>
In order to work with the newer <code>s3a://</code> protocol also set the values for <code>spark.hadoop.fs.s3a.impl</code> and <code>spark.hadoop.fs.s3a.endpoint </code>.
In addition, to support v4 of the S3 api be sure to pass the <code>-Dcom.amazonaws.services.s3.enableV4</code> driver options
for the config key <code>spark.driver.extraJavaOptions </code>
For instructions on how to configure <code>s3n://</code> check the hadoop documentation:
<a href="https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Authentication_properties">s3n authentication properties</a>
</p>


<h3>See Also</h3>

<p>Other Spark serialization routines: <code><a href="spark_load_table.html">spark_load_table</a></code>,
<code><a href="spark_read_csv.html">spark_read_csv</a></code>,
<code><a href="spark_read_jdbc.html">spark_read_jdbc</a></code>,
<code><a href="spark_read_json.html">spark_read_json</a></code>,
<code><a href="spark_read_libsvm.html">spark_read_libsvm</a></code>,
<code><a href="spark_read_orc.html">spark_read_orc</a></code>,
<code><a href="spark_read_parquet.html">spark_read_parquet</a></code>,
<code><a href="spark_read_source.html">spark_read_source</a></code>,
<code><a href="spark_read_table.html">spark_read_table</a></code>,
<code><a href="spark_save_table.html">spark_save_table</a></code>,
<code><a href="spark_write_csv.html">spark_write_csv</a></code>,
<code><a href="spark_write_jdbc.html">spark_write_jdbc</a></code>,
<code><a href="spark_write_json.html">spark_write_json</a></code>,
<code><a href="spark_write_orc.html">spark_write_orc</a></code>,
<code><a href="spark_write_parquet.html">spark_write_parquet</a></code>,
<code><a href="spark_write_source.html">spark_write_source</a></code>,
<code><a href="spark_write_table.html">spark_write_table</a></code>,
<code><a href="spark_write_text.html">spark_write_text</a></code>
</p>

<hr /><div style="text-align: center;">[Package <em>sparklyr</em> version 1.0.1 <a href="00Index.html">Index</a>]</div>
</body></html>
